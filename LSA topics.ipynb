{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libs\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.stem.snowball import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pymorphy2\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read file\n",
    "file_path = None\n",
    "text_column = None\n",
    "a=pd.read_csv(file_path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "stemmer = SnowballStemmer('russian')\n",
    "\n",
    "def tfidf_clean_text(text):\n",
    "    text=text.lower()\n",
    "    text = BeautifulSoup(text,'lxml').text\n",
    "    text = re.sub(r'\\|\\|\\|',r' ', text)\n",
    "    text = re.sub(r'http\\S+',r'<URL>', text)\n",
    "    text = text.replace('x','')\n",
    "    text = text.replace('\\\\n',' ')\n",
    "    text = text.replace('%',' <проценты>')\n",
    "    return text\n",
    "\n",
    "my_stopwords_rus=[]\n",
    "\n",
    "with open('../my_stopwords_rus.txt', encoding = \"cp1251\") as file:\n",
    "    my_stopwords_rus = [line.strip() for line in file]\n",
    "    \n",
    "morph= pymorphy2.MorphAnalyzer()\n",
    "    \n",
    "def tfidf_preprocess(text):\n",
    "    result = []\n",
    "    text = tfidf_clean_text(text)\n",
    "    for token in gensim.utils.simple_preprocess(text,min_len=2,max_len=30):\n",
    "        if token not in stopwords.words('russian'):\n",
    "                norm=morph.parse(token)[0].normal_form\n",
    "                if norm not in my_stopwords_rus and norm  not in stopwords.words('russian'):\n",
    "                    result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorization\n",
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf= TfidfVectorizer( use_idf=True, tokenizer=tfidf_preprocess, analyzer='word', ngram_range=(2,5), max_df=0.9, min_df=10)\n",
    "tfidf.fit(a[text_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parallellized transformation\n",
    "%%time\n",
    "import multiprocessing as mp\n",
    "import scipy.sparse as sp\n",
    "\n",
    "num_partitions=176\n",
    "num_workers=60\n",
    "\n",
    "def parallelize_dataframe(df,func):\n",
    "    df_split = np.array_split(df,num_partitions)\n",
    "    del df\n",
    "    pool = mp.Pool(num_workers)\n",
    "    print('Start mapping')\n",
    "    df =sp.vstack(pool.map(func,df_split),format='csr')\n",
    "    print('Concat together')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def func(df):\n",
    "    print('Apply to partition')\n",
    "    tfidf_matrix = tfidf.transform(df[text_column])\n",
    "    return tfidf_matrix\n",
    "\n",
    "X = parallelize_dataframe(a,func)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA aka SVD topic extraction\n",
    "%%time\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=9,algorithm='randomized',n_iter=50,random_state=22)\n",
    "svd_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top terms for every topic\n",
    "terms = tfidf.get_feature_names()\n",
    "topics=[]\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms,comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1],reverse=True)[:50]\n",
    "    topics.append({})\n",
    "    print('Topic '+str(i)+': ')\n",
    "    for t in sorted_terms:\n",
    "        topics[i][t[0]]=t[1]\n",
    "        print(t[0])\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud generation\n",
    "\n",
    "\n",
    "mask = np.array(Image.open(\"borders2.png\"))\n",
    "wordcloud = WordCloud(width=1000,height=1000, max_words=50, background_color='white', colormap='plasma',mask=mask).generate_from_frequencies(topics[8])\n",
    "\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
