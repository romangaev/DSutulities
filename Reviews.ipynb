{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libs\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score,balanced_accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pymorphy2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import shuffle\n",
    "import itertools\n",
    "\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def clean_text(text):\n",
    "    header = \"\"\n",
    "    if \"Заголовок:\" in text:\n",
    "        header = text.split(\"Заголовок:\")[1].split(\"Оценка:\")[0]\n",
    "        \n",
    "    body = text.split(\"Текст\")[1].split(\"Адрес отзыва:\")[0].split('Aдрес вопроса:')[0]\n",
    "    text = header+body\n",
    "\n",
    "    text = BeautifulSoup(text,'lxml').text\n",
    "    text = re.sub(r'\\|\\|\\|',r' ', text)\n",
    "    text = re.sub(r'http\\S+',r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x','')\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('%',' <проценты>')\n",
    "    return text\n",
    "\n",
    "stemmer = SnowballStemmer('russian')\n",
    "date_list=['года','месяца','января','февраля','марта','апреля','мая','июня','июля','августа','сентября','октября','ноября','декабря',\n",
    "          'месяце','январе','феврале','марте','апреле','мае','июне','июле','августе','сентябре','октябре','ноябре','декабре']\n",
    "rubles_list=['руб']\n",
    "location_list=['ул','улица','кор','пр','пр-кт','проезд',\"проспект\",'гор',\"пр-т\"]\n",
    "\n",
    "my_stopwords_rus=[]\n",
    "with open('my_stopwords_rus.txt', encoding = \"cp1251\") as file:\n",
    "    my_stopwords_rus = [line.strip() for line in file]\n",
    "    \n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    text = clean_text(text)\n",
    "    for token in gensim.utils.simple_preprocess(text,min_len=2,max_len=30):\n",
    "        if token not in stopwords.words('russian') and token not in my_stopwords_rus:\n",
    "            if token in date_list:\n",
    "                token='<дата>'\n",
    "            elif token in rubles_list:\n",
    "                token='рублей'\n",
    "            elif token in location_list:\n",
    "                token='<локация>'\n",
    "            #stemmed = stemmer.stem(WordNetLemmatizer().lemmatize(token,pos='v'))\n",
    "            #result.append(stemmed)\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def get_len(a):\n",
    "    b=len(a)\n",
    "    k=0.001\n",
    "    return b*k\n",
    "\n",
    "def tfidf_clean_text(text):\n",
    "    text=text.lower()\n",
    "    text = BeautifulSoup(text,'lxml').text\n",
    "    text = re.sub(r'\\|\\|\\|',r' ', text)\n",
    "    text = re.sub(r'http\\S+',r'<URL>', text)\n",
    "    text = text.replace('x','')\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('%',' <проценты>')\n",
    "    return text\n",
    "\n",
    "morph= pymorphy2.MorphAnalyzer()\n",
    "    \n",
    "def tfidf_preprocess(text):\n",
    "    result = []\n",
    "    text = tfidf_clean_text(text)\n",
    "    for token in gensim.utils.simple_preprocess(text,min_len=2,max_len=30):\n",
    "        if token not in stopwords.words('russian') and token not in my_stopwords_rus:\n",
    "            if token in date_list:\n",
    "                token='<дата>'\n",
    "                result.append(token)\n",
    "            elif token in rubles_list:\n",
    "                token='рублей'\n",
    "                result.append(token)\n",
    "            elif token in location_list:\n",
    "                token='<локация>'\n",
    "                result.append(token)\n",
    "            else:\n",
    "                #stemmed = stemmer.stem(WordNetLemmatizer().lemmatize(token,pos='v'))\n",
    "                #result.append(stemmed)\n",
    "                norm=morph.parse(token)[0].normal_form\n",
    "                if norm not in stopwords.words('russian') and norm not in my_stopwords_rus:\n",
    "                    result.append(norm)\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    plt.rcParams.update({'font.size':22})\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path=None\n",
    "data=pd.read_csv(file_path,error_bad_lines=False, encoding = \"cp1251\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data['PROCESSED']=data['MSG_TEXT'].apply(preprocess)\n",
    "data['PROCESSED_LEMMA']=data['MSG_TEXT'].apply(tfidf_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf= TfidfVectorizer(use_idf=True, tokenizer=tfidf_preprocess, analyzer='word', stop_words = my_stopwords_rus, ngram_range=(1,2),max_df=0.8,min_df=5)\n",
    "tfidf.fit(data['MSG_TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tf_train = tfidf.transform(data['MSG_TEXT'])\n",
    "y_tf_train = data[\"CLASS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=300,random_state=42)\n",
    "x_tf_train_SVD = svd.fit_transform(x_tf_train)\n",
    "x_tf_final = pd.DataFrame({'VEC':list(x_tf_train_SVD),'NAR_REI':data['NAR_REI'],'MARK':data['MARK']})\n",
    "x_tf_final = x_tf_final.values\n",
    "x_tf_final = [np.append(x[0],[x[1],x[2]]) for x in x_tf_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tf_test_SVD = svd.fit_transform(tfidf.transform(test_dataset['MSG_TEXT']))\n",
    "x_tf_test_final = pd.DataFrame({'VEC':list(x_tf_test_SVD),'NAR_REI':test_dataset['NAR_REI'],'MARK':test_dataset['MARK']})\n",
    "x_tf_test_final = x_tf_test_final.values\n",
    "x_tf_test_final = [np.append(x[0],[x[1],x[2]]) for x in x_tf_test_final]\n",
    "y_tf_test = test_dataset[\"CLASS\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Tf-Idf. CV LogReg, CV XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression(penalty='l2', solver='lbfgs',n_jobs=-1, multi_class=\"multinomial\")\n",
    "logreg_params={'C':[1e5]}\n",
    "best_logreg = GridSearchCV(logreg,logreg_params,cv=10,n_jobs=-1,refit ='balanced_accuracy',verbose=True, scoring=['balanced_accuracy','f1_macro'])\n",
    "best_logreg.fit(x_tf_final,y_tf_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logreg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logreg.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb = XGBClassifier(learning_rate=0.1,n_estimators=150,max_depth=10,min_child_weight=7,gamma=0,subsample=0.8,colsample_bytree=0.8,reg_alpha=1e-05)\n",
    "xgb_params={}\n",
    "\n",
    "best_xgb = GridSearchCV(xgb,xgb_params,cv=10,n_jobs=-1,refit ='balanced_accuracy',verbose=True, scoring=['balanced_accuracy','f1_macro'])\n",
    "best_xgb.fit(np.asarray(x_tf_final),y_tf_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['ID']=data.index.values\n",
    "\n",
    "data_tagged = data.apply(lambda r: TaggedDocument(r['PROCESSED'], tags=[r.CLASS]), axis=1)\n",
    "doc2vec_tagged = data.apply(lambda r: TaggedDocument(r['PROCESSED'], tags=[r.ID]), axis=1)\n",
    "\n",
    "\n",
    "cores= multiprocessing.cpu_count()\n",
    "\n",
    "model_dbow = Doc2Vec(doc2vec_tagged.values, dm=0, window=5, vector_size=300,negative=5,hs=0,min_count=5,sample=1e-5,workers=cores)\n",
    "doc_list=doc2vec_tagged.values[:]\n",
    "shuffle(doc_list)\n",
    "model_dbow.train(doc_list, total_examples=len(doc2vec_tagged.values), epochs=30)\n",
    "\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets,regressors = zip(*[(doc.tags[0],model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "y_all,x_all = vec_for_learning(model_dbow, data_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_all = pd.DataFrame({'VEC':list(x_all),'NAR_REI':data['NAR_REI'],'MARK':data['MARK']})\n",
    "x_all = x_all.values\n",
    "x_all = [np.append(x[0],[x[1],x[2]]) for x in x_all]\n",
    "\n",
    "x_test = [model_dbow.infer_vector(x) for x in test_dataset['PROCESSED']]\n",
    "x_test = pd.DataFrame({'VEC':list(x_test),'NAR_REI':test_dataset['NAR_REI'],'MARK':test_dataset['MARK']})\n",
    "x_test = x_test.values\n",
    "x_test = [np.append(x[0],[x[1],x[2]]) for x in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Doc2Vec. CV LogReg, CV XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression(penalty='l2', solver='lbfgs',n_jobs=-1, multi_class=\"multinomial\")\n",
    "logreg_params={'C':[1e5]}\n",
    "best_logreg = GridSearchCV(logreg,logreg_params,cv=10,n_jobs=-1,refit ='balanced_accuracy',verbose=True, scoring=['balanced_accuracy','f1_macro'])\n",
    "best_logreg.fit(x_all,y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logreg.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb = XGBClassifier(learning_rate=0.1,n_estimators=150,max_depth=10,min_child_weight=7,gamma=0,subsample=0.8,colsample_bytree=0.8,reg_alpha=1e-05)\n",
    "xgb_params={}\n",
    "best_xgb = GridSearchCV(xgb,xgb_params,cv=10,n_jobs=-1,refit ='balanced_accuracy',verbose=True, scoring=['balanced_accuracy','f1_macro'])\n",
    "best_xgb.fit(np.asarray(x_tf_final),y_tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Doc2Vec. Test LogReg, Test XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "np_x_all=np.array(x_all)\n",
    "np_y_all=np.array(y_all)\n",
    "\n",
    "sampler = SMOTE()\n",
    "res_x,res_y = sampler.fit_resample(np_x_all,np_y_all)\n",
    "\n",
    "classifier = LogisticRegression(C=1e5, penalty='l2', solver='lbfgs',n_jobs=-1, multi_class=\"multinomial\")\n",
    "classifier.fit(res_x,res_y)\n",
    "y_test_pred = classifier.predict(x_test)\n",
    "\n",
    "print (\"F1 W: {}\".format(f1_score(y_tf_test,y_test_pred,average=\"weighted\")))\n",
    "print (\"F1 MAC: {}\".format(f1_score(y_tf_test,y_test_pred,average=\"macro\")))\n",
    "print (\"B ACC: {}\".format(balanced_accuracy_score(y_tf_test,y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier = XGBClassifier(max_depth=10,min_child_weight=7,learning_rate=0.1,n_estimators=150,seed=0,subsample=0.8,colsample_bytree=0.8,objective='reg:logistic',n_jobs=-1)\n",
    "classifier.fit(res_x,res_y)\n",
    "y_test_pred = classifier.predict(x_test)\n",
    "\n",
    "print (\"F1 W: {}\".format(f1_score(y_tf_test,y_test_pred,average=\"weighted\")))\n",
    "print (\"F1 MAC: {}\".format(f1_score(y_tf_test,y_test_pred,average=\"macro\")))\n",
    "print (\"B ACC: {}\".format(balanced_accuracy_score(y_tf_test,y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Doc2Vec Test XGB Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rc('xtick',labelsize=20)\n",
    "matplotlib.rc('ytick',labelsize=20)\n",
    "class_names=test_dataset['CLASS'].unique().tolist()\n",
    "print(class_names)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_true=y_tf_test,y_pred=y_test_pred,labels=class_names)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
