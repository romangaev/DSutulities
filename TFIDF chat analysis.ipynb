{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cx_Oracle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import gc\n",
    "\n",
    "#Set up DB connection\n",
    "os.environ['ORACLE_HOME'] = \"oraclepath\"\n",
    "dns_tns = cx_Oracle.makedsn('ip','7777',service_name = 'servicename')\n",
    "usr = getpass.getpass(prompt='Insert username:\\n')\n",
    "pwd = getpass.getpass(prompt='Insert password:\\n')\n",
    "conn = cx_Oracle.connect(user=usr, password=pwd, dsn=dns_tns, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.read_sql('''''',con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.stem.snowball import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pymorphy2\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "stemmer = SnowballStemmer('russian')\n",
    "date_list=['года','месяца','января','февраля','марта','апреля','мая','июня','июля','августа','сентября','октября','ноября','декабря',\n",
    "          'месяце','январе','феврале','марте','апреле','мае','июне','июле','августе','сентябре','октябре','ноябре','декабре']\n",
    "rubles_list=['руб']\n",
    "location_list=['ул','улица','кор','пр','пр-кт','проезд',\"проспект\",'гор',\"пр-т\"]\n",
    "\n",
    "def tfidf_clean_text(text):\n",
    "    text=text.lower()\n",
    "    text = BeautifulSoup(text,'lxml').text\n",
    "    text = re.sub(r'\\|\\|\\|',r' ', text)\n",
    "    text = re.sub(r'http\\S+',r'<URL>', text)\n",
    "    text = text.replace('x','')\n",
    "    text = text.replace('\\\\n',' ')\n",
    "    text = text.replace('%',' <проценты>')\n",
    "    return text\n",
    "\n",
    "morph= pymorphy2.MorphAnalyzer()\n",
    "    \n",
    "def tfidf_preprocess(text):\n",
    "    result = []\n",
    "    text = tfidf_clean_text(text)\n",
    "    for token in gensim.utils.simple_preprocess(text,min_len=2,max_len=30):\n",
    "        if token not in stopwords.words('russian') :\n",
    "            if token in date_list:\n",
    "                token='<дата>'\n",
    "                result.append(token)\n",
    "            elif token in rubles_list:\n",
    "                token='рублей'\n",
    "                result.append(token)\n",
    "            elif token in location_list:\n",
    "                token='<локация>'\n",
    "                result.append(token)\n",
    "            else:\n",
    "                #stemmed = stemmer.stem(WordNetLemmatizer().lemmatize(token,pos='v'))\n",
    "                #result.append(stemmed)\n",
    "                norm=morph.parse(token)[0].normal_form\n",
    "                if norm not in stopwords.words('russian'):\n",
    "                    result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf= TfidfVectorizer( use_idf=True, tokenizer=tfidf_preprocess, analyzer='word', ngram_range=(1,2), max_df=0.8, min_df=5)\n",
    "tfidf.fit(logs['C_SCRIPT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x_tf_train = tfidf.transform(logs['C_SCRIPT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import multiprocessing as mp\n",
    "import scipy.sparse as sp\n",
    "\n",
    "num_partitions=36\n",
    "num_workers=18\n",
    "\n",
    "def parallelize_dataframe(df,func):\n",
    "    df_split = np.array_split(df,num_partitions)\n",
    "    del df\n",
    "    pool = mp.Pool(num_workers)\n",
    "    print('Start mapping')\n",
    "    df =sp.vstack(pool.map(func,df_split),format='csr')\n",
    "    print('Concat together')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def func(df):\n",
    "    print('Apply to partition')\n",
    "    tfidf_matrix = tfidf.transform(df['C_SCRIPT'])\n",
    "    return tfidf_matrix\n",
    "\n",
    "x_tf_train_parallel = parallelize_dataframe(logs,func)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tf_train = logs['OPERATOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=-1)\n",
    "acc=cross_val_score(logreg,x_tf_train,y_tf_train,scoring='f1_macro',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(x_tf_train,y_tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=30\n",
    "feature_names = tfidf.get_feature_names()\n",
    "coefs_with_fns = sorted(zip(logreg.coef_[0],feature_names))\n",
    "top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n+1):-1])\n",
    "for (coef_1,fn_1), (coef_2,fn_2) in top:\n",
    "    print (\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" %(coef_1,fn_1,coef_2,fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toplot={v:k for k,v in coefs_with_fns[-100:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "mask = np.array(Image.open(\"borders.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=1000,height=1000, max_words=60, background_color='white', colormap='plasma',mask=mask).generate_from_frequencies(toplot)\n",
    "\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
